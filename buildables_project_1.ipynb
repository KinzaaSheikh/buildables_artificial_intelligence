{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRXPkU8GO6ynN9hjdpdCW2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KinzaaSheikh/buildables_artificial_intelligence/blob/main/buildables_project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ],
      "metadata": {
        "id": "0eQPKjQFXczT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Give a recap of notebook 1\n",
        "\n",
        "üëâ In notebook 1 the focus was on some theory of what LLMs are, Agentic AI, and other background information. It was mostly about you getting a feel of the understanding we had of AI and how much ground do we need to cover.\n",
        "\n",
        "The coding part was simply integrating API keys in the Colab environment and testing whether it is working.\n",
        "\n",
        "\n",
        "## Give a recap of notebook 2\n",
        "\n",
        "üëâ In notebook 2 we really got our hands dirty with NLP tasks, task preprocessing, learning a little bit of AI workflow and got a glimplse on how chatbots can be created.\n",
        "\n",
        "## Give a recap of notebook 3\n",
        "\n",
        "üëâ This week we were allowed to play around with prompting on our own."
      ],
      "metadata": {
        "id": "gE3T4yiqVi-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìö Chatbot Development Project"
      ],
      "metadata": {
        "id": "dTE8MhzaWPd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1) Use case ‚Äî **Arabic grammar & vocabulary tutor for elder stroke survivors**\n",
        "\n",
        "**Why I chose it (short):**\n",
        "Elder stroke survivors often face language impairment and need gentle, repetitive, scaffolded practice to (re)learn grammar and vocabulary. A patient chatbot can give short lessons, graded exercises, immediate corrective feedback, and encouragement anytime ‚Äî helping preserve purpose and providing cognitively appropriate practice.\n",
        "\n",
        "**Target users & expected benefits**\n",
        "\n",
        "* Primary: older adult Muslim stroke survivors who can read Arabic but need grammar/vocab practice (and caregivers supporting them).\n",
        "* Benefits: short, repeatable lessons; adjustable difficulty; immediate corrective feedback; consistent encouragement; accessible pace (short messages, transliteration), 24/7 availability, low-cost scaling for clinics/centers.\n",
        "\n",
        "**Limitations (what it cannot do)**\n",
        "\n",
        "* Not a medical or speech-language pathology replacement (no clinical diagnosis or therapy plans).\n",
        "* Will sometimes hallucinate details (dates/factual claims) ‚Äî must not be used for medical/legal advice.\n",
        "* Not a substitute for a trained SLP for severe aphasia; escalate to professionals when needed.\n",
        "* Offline use requires local deployment or on-prem inference; by default this design uses Groq cloud inference.\n",
        "\n",
        "---\n",
        "\n",
        "# 2) Why worth solving with a chatbot ‚Äî justification\n",
        "\n",
        "* **Accessibility & repetition:** Tutors can offer short, repeatable drills tailored to cognitive load; websites/apps are often less conversational and less forgiving in pacing.\n",
        "* **Personalization:** Chatbots can adapt level and style instantly (e.g., simpler sentences, transliteration, larger fonts).\n",
        "* **Low-cost availability:** A single model instance can serve many learners; human tutors are limited/expensive.\n",
        "* **Immediate feedback loop:** Users get corrections and examples right away ‚Äî crucial for retention.\n",
        "\n",
        "**Specific tasks the chatbot will perform**\n",
        "\n",
        "* Explain one grammar point in a single short paragraph (and provide simple Arabic example + transliteration).\n",
        "* Provide 1‚Äì2 short practice exercises and a model answer.\n",
        "* Give short, constructive feedback on user answers.\n",
        "* Track progress per session (short-term) and optionally across sessions (persistent profile).\n",
        "* Offer to repeat/slow down or change difficulty.\n",
        "\n",
        "---\n",
        "\n",
        "# 3) Select the LLM (Groq) ‚Äî research-based pick + reasons\n",
        "\n",
        "**Which models Groq supports (high-level):** Groq hosts Meta Llama 3 variants (8B and 70B), Mixtral (mixtral-8x7b), Gemma (7B), and others. ([docs.typingmind.com][1])\n",
        "\n",
        "**Important model facts used in selection**\n",
        "\n",
        "* Groq production Llama 3 models include `llama-3.1-8b-instant` (very low latency, large context). ([GroqCloud][2])\n",
        "* Groq emphasizes ultra-low inference latency (deterministic LPU) ‚Äî good for responsive chat. ([Groq][3])\n",
        "* Groq provides a Python SDK with OpenAI-compatible chat completions. Quickstart examples exist. ([GroqCloud][4])\n",
        "\n",
        "**Final model decision & rationale**\n",
        "\n",
        "* **Primary model:** `llama-3.1-8b-instant` ‚Äî chosen for **speed, cost, and adequate instruction-following** for tutoring tasks. It supports very large context windows (useful for session transcripts and lesson content) and is a production model on Groq. ([GroqCloud][2])\n",
        "* **Fallback/high-quality option (optional):** `llama-3.3-70b-versatile` ‚Äî use when you want the highest possible accuracy for complex, detailed explanations (tradeoff: higher cost). ([GroqCloud][2])\n",
        "\n",
        "**Why not the smallest gemma/mixtral by default?**\n",
        "Mixtral/Gemma are strong but Llama 3 is instruction-tuned and tends to perform better for careful language explanations and alignment; for an empathetic tutor persona and multilingual needs Llama 3 is a safe, well-documented choice on Groq. ([docs.litellm.ai][5])\n",
        "\n",
        "---\n",
        "\n",
        "# 4) Conversation flow (design + examples)\n",
        "\n",
        "**High-level policies**\n",
        "\n",
        "* Keep replies short (2‚Äì4 sentences) unless the user asks for more.\n",
        "* Use **English + Arabic script + simple transliteration** for examples.\n",
        "* Always give *one* short example, *one* practice exercise, and a short rubric for feedback.\n",
        "* Encourage and offer to repeat or slow down.\n",
        "\n",
        "**Session memory**\n",
        "\n",
        "* Short-term (in-memory): last N turns (e.g., last 6 user-assistant exchanges) to maintain context in a session.\n",
        "* Optional persistent memory: user preferences (preferred difficulty, transliteration on/off) stored in DB (Redis/Postgres) ‚Äî only if user opts in.\n",
        "\n",
        "**Greeting and goodbye**\n",
        "\n",
        "* Greeting: ‚ÄúSalam ‚Äî I‚Äôm Noor, your Arabic tutor. What would you like to practice for 5‚Äì10 minutes today?‚Äù (Offer options: grammar/vocab/quiz.)\n",
        "* Goodbye: ‚ÄúGreat work today ‚Äî would you like to save this short lesson? See you next time!‚Äù\n",
        "\n",
        "**Unknown questions / fallback**\n",
        "\n",
        "* If the assistant is unsure: short fallback ‚Äî ‚ÄúI‚Äôm not sure about that. Would you like a simple example instead, or should I ask a specialist?‚Äù\n",
        "* If user requests medical or clinical advice: refuse and recommend a qualified professional.\n",
        "\n",
        "**Sample user inputs & expected assistant outputs**\n",
        "\n",
        "1. User: ‚ÄúExplain the Arabic dual (ÿßŸÑŸÖÿ´ŸÜŸâ) briefly.‚Äù\n",
        "   Assistant: short rule in English, Arabic example + transliteration, 1 quick exercise (convert singular to dual), and a simple model answer.\n",
        "\n",
        "2. User: ‚ÄúI wrote: ŸÉÿ™ÿßÿ®ÿßŸÜ ‚Äî is that correct for two books?‚Äù\n",
        "   Assistant: Yes/No correction, highlight pattern (ending with ŸÄÿßŸÜ in nominative), one short corrective hint.\n",
        "\n",
        "3. User: ‚ÄúI want an easy quiz.‚Äù\n",
        "   Assistant: 3 1-word prompts; user types answers; assistant grades and gives short encouragement.\n",
        "\n",
        "---\n",
        "\n",
        "# 5) Prompting technique ‚Äî chosen method & **actual template**\n",
        "\n",
        "**Technique(s) used**\n",
        "\n",
        "* **System prompt + few-shot examples (few-shot prompting)** ‚Äî gives consistent persona and output format.\n",
        "* Use **structured output** (JSON-like or clearly delimited sections) so the UI can parse rule/example/exercise.\n",
        "* Keep temperature low (0.2‚Äì0.4) for correctness and low hallucination.\n",
        "\n",
        "**Why few-shot + system prompt?**\n",
        "\n",
        "* For tutoring you want a predictable output format (rule + example + exercise + feedback). Few-shot examples teach that format clearly and robustly. System prompt sets persona (patient tutor), language style, pacing.\n",
        "\n",
        "**Sample system prompt + two few-shot examples (copy/paste-friendly)**\n",
        "\n",
        "```\n",
        "SYSTEM:\n",
        "You are Noor, a patient, empathetic Arabic grammar tutor for older adult learners.\n",
        "Rules:\n",
        "- Keep each reply short (max 4 sentences for explanation).\n",
        "- Provide: [Rule], [Example (Arabic + transliteration)], [Exercise], [Model answer], [Tip].\n",
        "- Use simple English to explain grammar; show Arabic script and transliteration for examples.\n",
        "- Always offer an option: \"Would you like to try an exercise?\" at the end.\n",
        "\n",
        "FEW-SHOT EXAMPLES:\n",
        "\n",
        "USER: \"Explain the Arabic dual (ÿßŸÑŸÖÿ´ŸÜŸâ).\"\n",
        "ASSISTANT:\n",
        "[Rule] The dual (ÿßŸÑŸÖÿ´ŸÜŸâ) expresses exactly two items. In the nominative, add \"ÿßŸÜŸê\" or \"ÿßŸÜ\" (ŸÄÿßŸÜŸê / ŸÄÿßŸÜ).\n",
        "[Example] ŸÉÿ™ÿßÿ®ÿßŸÜ ‚Äî kitƒÅbƒÅn (two books)\n",
        "[Exercise] Convert \"ŸÇŸÑŸÖ\" to the dual.\n",
        "[Model answer] ŸÇŸÑŸÖÿßŸÜ ‚Äî qalamƒÅn\n",
        "[Tip] Focus on the ending: ŸÄÿßŸÜ for two in a simple sentence.\n",
        "Would you like to try an exercise?\n",
        "\n",
        "USER: \"How do you make a feminine plural for 'teacher'?\"\n",
        "ASSISTANT:\n",
        "[Rule] To make feminine plural (for people) often add \"ÿßÿ™\" (ŸÄÿßÿ™).\n",
        "[Example] ŸÖÿπŸÑŸÖÿ© ‚Äî mu øallimah (female teacher); ŸÖÿπŸÑŸÖÿßÿ™ ‚Äî mu øallimƒÅt (female teachers)\n",
        "[Exercise] Convert \"ŸÖŸÖÿ±ÿ∂ÿ©\" to plural.\n",
        "[Model answer] ŸÖŸÖÿ±ÿ∂ÿßÿ™ ‚Äî mumarri·∏çƒÅt\n",
        "[Tip] Watch the short vowels when pronouncing.\n",
        "Would you like another example?\n",
        "```\n",
        "\n",
        "Use this system + few-shot block as the top of `messages` in the API call, then append user messages and recent session history.\n",
        "\n",
        "---\n",
        "\n",
        "# 6) Minimal runnable implementation (FastAPI backend + tiny web UI)\n",
        "\n",
        "Below is a minimal, production-directional example you can run locally. It demonstrates:\n",
        "\n",
        "* Groq Python SDK usage (chat completions)\n",
        "* Short-term in-memory session memory (for demo only ‚Äî replace with Redis/Postgres in prod)\n",
        "* Structured assistant output parsing\n",
        "\n",
        "> **Prereqs:** Python 3.9+, `pip install fastapi uvicorn groq python-multipart`\n",
        "> Set environment var: `export GROQ_API_KEY=\"YOUR_KEY_HERE\"`\n",
        "\n",
        "### `app.py` (FastAPI backend)\n",
        "\n",
        "```python\n",
        "# app.py\n",
        "import os\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from groq import Groq\n",
        "\n",
        "# Basic config\n",
        "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
        "if not GROQ_API_KEY:\n",
        "    raise RuntimeError(\"Set GROQ_API_KEY environment variable\")\n",
        "\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "MODEL = \"llama-3.1-8b-instant\"   # chosen for speed / cost tradeoff\n",
        "\n",
        "app = FastAPI(title=\"Noor - Arabic Tutor (Groq)\")\n",
        "\n",
        "# In-memory sessions (demo). Use Redis/Postgres for production.\n",
        "sessions = {}  # session_id -> list of message dicts {\"role\":..., \"content\":...}\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    session_id: str\n",
        "    message: str\n",
        "    user_name: str | None = None\n",
        "\n",
        "# Basic system prompt (keeps persona and format)\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are Noor, a patient Arabic grammar tutor for older adult learners. \"\n",
        "    \"Always keep answers short and friendly. \"\n",
        "    \"Format responses as sections labeled [Rule], [Example], [Exercise], [Model answer], [Tip]. \"\n",
        "    \"Provide Arabic script and simple transliteration for examples. \"\n",
        "    \"End with: 'Would you like to try an exercise?'\\n\"\n",
        ")\n",
        "\n",
        "# Optional few-shot: include 1-2 short examples to lock format\n",
        "FEW_SHOT = [\n",
        "    {\"role\": \"assistant\", \"content\":\n",
        "        \"[Rule] The dual (ÿßŸÑŸÖÿ´ŸÜŸâ) denotes two items.\\n\"\n",
        "        \"[Example] ŸÉÿ™ÿßÿ®ÿßŸÜ ‚Äî kitƒÅbƒÅn (two books)\\n\"\n",
        "        \"[Exercise] Convert 'ŸÇŸÑŸÖ' to dual.\\n\"\n",
        "        \"[Model answer] ŸÇŸÑŸÖÿßŸÜ ‚Äî qalamƒÅn\\n\"\n",
        "        \"[Tip] Look for the ending ŸÄÿßŸÜ for two.\\n\"\n",
        "        \"Would you like to try an exercise?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_endpoint(req: ChatRequest):\n",
        "    # Build session history (keep small window)\n",
        "    history = sessions.setdefault(req.session_id, [])\n",
        "    # Build messages: system + few-shot + recent history + user\n",
        "    messages = [{\"role\":\"system\", \"content\": SYSTEM_PROMPT}] + FEW_SHOT\n",
        "    # include last 6 messages to keep tokens manageable\n",
        "    messages += history[-6:]\n",
        "    messages.append({\"role\":\"user\", \"content\": req.message})\n",
        "\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=messages,\n",
        "            temperature=0.25,\n",
        "            max_tokens=512,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "    assistant_text = completion.choices[0].message.content\n",
        "\n",
        "    # Simple fallback-on-uncertainty: if model hedges, offer to rephrase\n",
        "    hedges = [\"i don't know\", \"i'm not sure\", \"i could be wrong\"]\n",
        "    if any(h in assistant_text.lower() for h in hedges):\n",
        "        assistant_text = (\n",
        "            \"Sorry, I don't have a confident short explanation for that. \"\n",
        "            \"Would you like a simpler example instead, or to ask a human tutor?\"\n",
        "        )\n",
        "\n",
        "    # Append user+assistant to session history\n",
        "    history.append({\"role\":\"user\", \"content\": req.message})\n",
        "    history.append({\"role\":\"assistant\", \"content\": assistant_text})\n",
        "\n",
        "    return {\"reply\": assistant_text}\n",
        "```\n",
        "\n",
        "### Simple web UI (`index.html`)\n",
        "\n",
        "```html\n",
        "<!doctype html>\n",
        "<html>\n",
        "<head><meta charset=\"utf-8\"><title>Noor ‚Äî Arabic Tutor</title></head>\n",
        "<body>\n",
        "  <div style=\"max-width:600px;margin:20px auto;font-family:sans-serif\">\n",
        "    <h2>Noor ‚Äî Arabic Tutor (demo)</h2>\n",
        "    <div id=\"conv\" style=\"border:1px solid #ddd;padding:12px;height:300px;overflow:auto\"></div>\n",
        "    <input id=\"msg\" style=\"width:80%\" placeholder=\"Type a short question (e.g., 'Explain the dual')\" />\n",
        "    <button id=\"send\">Send</button>\n",
        "  </div>\n",
        "\n",
        "<script>\n",
        "const sessionId = \"demo-session-1\"; // in prod create unique per user\n",
        "document.getElementById(\"send\").onclick = async () => {\n",
        "  const msg = document.getElementById(\"msg\").value.trim();\n",
        "  if(!msg) return;\n",
        "  addLine(\"You: \" + msg);\n",
        "  document.getElementById(\"msg\").value = \"\";\n",
        "  const res = await fetch(\"/chat\", {\n",
        "    method: \"POST\",\n",
        "    headers: {\"Content-Type\":\"application/json\"},\n",
        "    body: JSON.stringify({session_id: sessionId, message: msg})\n",
        "  });\n",
        "  const data = await res.json();\n",
        "  addLine(\"Noor: \" + data.reply);\n",
        "};\n",
        "function addLine(text){\n",
        "  const el = document.getElementById(\"conv\");\n",
        "  el.innerHTML += \"<div style='margin:8px 0'>\"+text+\"</div>\";\n",
        "  el.scrollTop = el.scrollHeight;\n",
        "}\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "```\n",
        "\n",
        "**Run it locally**\n",
        "\n",
        "1. `export GROQ_API_KEY=\"...\"`\n",
        "2. `pip install fastapi uvicorn groq python-multipart`\n",
        "3. `uvicorn app:app --reload --port 8000`\n",
        "4. Open `http://localhost:8000/` (put `index.html` in same folder and serve it, or use a tiny static server).\n",
        "\n",
        "---\n",
        "\n",
        "# 7) Safety, evaluation & limitations in practice\n",
        "\n",
        "* **Hallucinations:** Use low temperature and structured prompts; still verify critical facts externally.\n",
        "* **Medical / clinical requests:** refuse and direct to professionals.\n",
        "* **Privacy:** store minimal personal data; use encryption at rest; follow HIPAA-like rules if used for health (consult legal).\n",
        "* **Monitoring:** log user ratings and sample transcripts (with consent) to fine-tune prompts and detect failures.\n",
        "\n",
        "---\n",
        "\n",
        "# 8) Deployment & scaling notes (brief)\n",
        "\n",
        "* For production, **replace in-memory sessions with Redis** and persist user preferences in a DB.\n",
        "* Use Groq's production models and autoscaling; Groq provides deterministic low latency LPUs for fast inference. ([Groq][3])\n",
        "* Containerize with Docker, use a load balancer and short-lived worker pods; monitor costs (Groq pricing page). ([Groq][6])\n",
        "\n",
        "---\n",
        "\n",
        "# 9) Sources & helpful links (most important citations)\n",
        "\n",
        "* Groq Models / production list (Llama 3 variants): Groq docs. ([GroqCloud][2])\n",
        "* Groq quickstart & Python SDK examples (how to call chat completions): Groq Quickstart / SDK. ([GroqCloud][4])\n",
        "* Groq supported upstream models list & examples (Llama 3, Mixtral, Gemma): third-party summaries & docs. ([docs.typingmind.com][1])\n",
        "* Groq speed / LPU / latency claims (for justification): groq.com pages & API benchmarking notes. ([Groq][3])\n",
        "\n",
        "---\n",
        "\n",
        "If you want I can:\n",
        "\n",
        "* Convert the UI into a **React + Tailwind** single-file component and put it in a canvas (previewable).\n",
        "* Add **audio input/output** (speech-to-text + TTS), slower but very useful for elders.\n",
        "* Add **persistence** example (Redis + Postgres) and a small Dockerfile / Kubernetes manifest.\n",
        "\n",
        "Which of those would you like next? (Or I can immediately produce the React/Tailwind code and a Dockerfile ‚Äî your call.)\n",
        "\n",
        "[1]: https://docs.typingmind.com/manage-and-connect-ai-models/groq-api-%28llama-3-mixtral-8x7b-gemma-7b%29?utm_source=chatgpt.com \"Groq API (LLaMA 3, Mixtral 8x7b, Gemma 7b)\"\n",
        "[2]: https://console.groq.com/docs/models?utm_source=chatgpt.com \"Supported Models - GroqDocs\"\n",
        "[3]: https://groq.com/?utm_source=chatgpt.com \"Groq is fast inference for AI builders\"\n",
        "[4]: https://console.groq.com/docs/quickstart?utm_source=chatgpt.com \"Quickstart - GroqDocs\"\n",
        "[5]: https://docs.litellm.ai/docs/providers/groq?utm_source=chatgpt.com \"Groq\"\n",
        "[6]: https://groq.com/pricing?utm_source=chatgpt.com \"On-demand Pricing for Tokens-as-a-Service\"\n"
      ],
      "metadata": {
        "id": "d_DKlWviZYaX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXrP3qfS2Am0"
      },
      "outputs": [],
      "source": []
    }
  ]
}